{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da91147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"aqar.csv\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d0301b",
   "metadata": {},
   "source": [
    "- [x] Use Y-data\n",
    "- [] try multi layer preceptron(loss-funtion MSE, opt: adamw, Activation: Leaky ReLU)\n",
    "- [x] try a Layered ML model(Use three diffrent ml models, they output to the forth)\n",
    "  - Didn't Work Had Worse R^2 Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07504435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=50, figsize=(20,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944597f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_feature_cols=[\"area_sqm\",\"num_bathrooms\",\"num_bedrooms\",\"num_rooms\"]\n",
    "cat_feature_cols=[\"location\",]\n",
    "bool_feature_cols=[\"lift\"]\n",
    "\n",
    "df : pd.DataFrame = df[(df['is_rental'] == False) & (df['is_daily_rental'] == False) & (df['sale_type'] != 'rent') & (df['sale_type'] !='daily')].copy()\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "# drop listings of land without buildings\n",
    "df : pd.DataFrame = df[df['category_ga_property_category'] != 'land'].copy()\n",
    "# drop listings of commercial buildings\n",
    "df : pd.DataFrame = df[(df[\"category_ga_listing_type\"]!= \"office\") & (df[\"category_ga_listing_type\"]!=\"store\") & (df[\"category_ga_listing_type\"]!=\"warehouse\") & (df[\"category_ga_listing_type\"]!=\"lounge\")].copy()\n",
    "\n",
    "for bool_col in bool_feature_cols:\n",
    "    df[bool_col] = df[bool_col].astype(int)\n",
    "df['location'] = df['city'] + '_' + df['district']\n",
    "\n",
    "target_col=[\"price\"]\n",
    "\n",
    "\n",
    "df[num_feature_cols + cat_feature_cols + bool_feature_cols + target_col].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25689c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[num_feature_cols + cat_feature_cols + bool_feature_cols + target_col].hist(bins=50, figsize=(20,15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26111c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "profile = ProfileReport(df, title=\"All Aqar Data Profiling Report\", explorative=True)\n",
    "profile.to_file(\"raw_aqar_data_profiling_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbb1c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4502 is the number of rows \n",
    "# 1933 is the number of missing values in num_bathrooms\n",
    "df[\"num_bedrooms\"].isna().sum()\n",
    "df[num_feature_cols + cat_feature_cols + bool_feature_cols + target_col].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9dc1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile Data\n",
    "profile = ProfileReport(df[num_feature_cols + cat_feature_cols + bool_feature_cols + target_col].copy(), title=\"Aqar Dataset Profiling Report\")\n",
    "profile.to_file(\"subset_aqar_data_profiling_report.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df09689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training and Evaluation before preprocessing pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, OneToOneFeatureMixin\n",
    "from sklearn.utils._set_output import _SetOutputMixin\n",
    "\n",
    "class RareLabelGrouper(OneToOneFeatureMixin, TransformerMixin, BaseEstimator):\n",
    "    _parameter_constraints = {}\n",
    "    \n",
    "    def __init__(self, tol=0.01, replace_with='Other'):\n",
    "        self.tol = tol\n",
    "        self.replace_with = replace_with\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Store feature names\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.feature_names_in_ = X.columns.to_numpy()\n",
    "        else:\n",
    "            self.feature_names_in_ = np.array([f\"x{i}\" for i in range(X.shape[1])])\n",
    "        \n",
    "        # Learn frequent labels\n",
    "        self.frequent_labels_ = {}\n",
    "        X_df = pd.DataFrame(X, columns=self.feature_names_in_)\n",
    "        for col in X_df.columns:\n",
    "            counts = pd.Series(X_df[col]).value_counts(normalize=True)\n",
    "            self.frequent_labels_[col] = counts[counts >= self.tol].index\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_df = pd.DataFrame(X, columns=self.feature_names_in_).copy()\n",
    "        for col in X_df.columns:\n",
    "            known_labels = self.frequent_labels_.get(col, [])\n",
    "            X_df[col] = X_df[col].where(X_df[col].isin(known_labels), self.replace_with)\n",
    "        return X_df\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Required for pandas output support\"\"\"\n",
    "        if input_features is None:\n",
    "            return self.feature_names_in_\n",
    "        return np.asarray(input_features, dtype=object)\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def compute_ratios(X):\n",
    "    X = X.copy()\n",
    "    # Add small epsilon to avoid division by zero\n",
    "    X['sqm_per_room'] = X['area_sqm'] / (X['num_rooms'].replace(0, 1))\n",
    "    X['bath_per_bed'] = X['num_bathrooms'] / (X['num_bedrooms'].replace(0, 1))\n",
    "    return X\n",
    "\n",
    "ratio_adder = FunctionTransformer(compute_ratios, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a16ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler, TargetEncoder, PolynomialFeatures\n",
    "\n",
    "pipe1= ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ]), num_feature_cols),\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\",sparse_output=False,min_frequency=0.001))\n",
    "    ]), cat_feature_cols),\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "])\n",
    "\n",
    "pipe1_optimized = ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"ratios\", ratio_adder),\n",
    "    ]), num_feature_cols),\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"rare_grouper\", RareLabelGrouper(tol=0.001, replace_with='other')), \n",
    "        (\"target_enc\", TargetEncoder())\n",
    "    ]), cat_feature_cols),\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "])\n",
    "\n",
    "pipe1_optimized_scaled = ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"ratios\", ratio_adder),\n",
    "        (\"scaler\", RobustScaler())\n",
    "    ]), num_feature_cols),\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"rare_grouper\", RareLabelGrouper(tol=0.001, replace_with='other')), \n",
    "        (\"target_enc\", TargetEncoder())\n",
    "    ]), cat_feature_cols),\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "])\n",
    "pipe2 = ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ]), num_feature_cols),\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"target_enc\", TargetEncoder())\n",
    "    ]), cat_feature_cols),\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "])\n",
    "\n",
    "pipe2_optimized = ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"ratios\", ratio_adder),\n",
    "    ]), num_feature_cols),\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"target_enc\", TargetEncoder())\n",
    "    ]), cat_feature_cols),\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "pipe3 = ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ]), num_feature_cols),\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"rare_grouper\", RareLabelGrouper(tol=0.001, replace_with='other')),\n",
    "        (\"target_enc\", TargetEncoder())\n",
    "    ]), cat_feature_cols),\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "])\n",
    "pipe4 = ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", RobustScaler())\n",
    "    ]), num_feature_cols),\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"target_enc\", TargetEncoder())\n",
    "    ]), cat_feature_cols),\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "])\n",
    "\n",
    "pipe5_ratio_scaled = ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"ratios\", ratio_adder),\n",
    "        (\"scaler\", RobustScaler())\n",
    "    ]), num_feature_cols),\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"target_enc\", TargetEncoder())\n",
    "    ]), cat_feature_cols),\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "])\n",
    "\n",
    "# With polynomial features (degree 2)\n",
    "pipe6_poly = ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"ratios\", ratio_adder),\n",
    "        (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        (\"scaler\", RobustScaler())\n",
    "    ]), num_feature_cols),\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"rare_grouper\", RareLabelGrouper(tol=0.001, replace_with='other')),\n",
    "        (\"target_enc\", TargetEncoder())\n",
    "    ]), cat_feature_cols),\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "])\n",
    "\n",
    "# Different rare label threshold\n",
    "pipe7_rare_moderate = ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"ratios\", ratio_adder),\n",
    "        (\"scaler\", RobustScaler())\n",
    "    ]), num_feature_cols),\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"rare_grouper\", RareLabelGrouper(tol=0.001, replace_with='other')),\n",
    "        (\"target_enc\", TargetEncoder())\n",
    "    ]), cat_feature_cols),\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "])\n",
    "\n",
    "\n",
    "catboost_pipe = ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ]), num_feature_cols),\n",
    "\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"rare_grouper\", RareLabelGrouper(tol=0.001, replace_with='other')),\n",
    "    ]), cat_feature_cols),\n",
    "\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "])\n",
    "\n",
    "catboost_pipe_nogroup = ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ]), num_feature_cols),\n",
    "\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    ]), cat_feature_cols),\n",
    "\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "])\n",
    "# Define features and target\n",
    "X = df[num_feature_cols + cat_feature_cols + bool_feature_cols]\n",
    "y = np.log1p(df[target_col])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c745a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "def build_cache_path(pipeline_name:str, model_name:str) -> Path:\n",
    "    return Path(f\"cache/{pipeline_name}_{model_name}_model.pkl\")\n",
    "\n",
    "def cache_model(model, pipeline_name:str, model_name:str):\n",
    "    cache_path = build_cache_path(pipeline_name, model_name)\n",
    "    cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(cache_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98cf6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor,HistGradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "cb=CatBoostRegressor(loss_function=\"RMSE\",\n",
    "            eval_metric=\"R2\",\n",
    "            random_seed=42,\n",
    "            verbose=False\n",
    ")\n",
    "\n",
    "def evaluate_model(pipe_name, pipe, model_name, best_pipeline):\n",
    "    result= {}\n",
    "    fit_params = {}\n",
    "    if model_name == \"CatBoost\":\n",
    "        pipe.fit(X_train, y_train)\n",
    "        feature_names = pipe.get_feature_names_out()\n",
    "        cat_features_idx = [\n",
    "            i for i, col in enumerate(feature_names) \n",
    "            if \"cat_pipeline__\" in col\n",
    "        ]\n",
    "        fit_params = {'model__cat_features': cat_features_idx}\n",
    "    \n",
    "    cv_results = cross_validate(\n",
    "        best_pipeline,\n",
    "        X_train,\n",
    "        y_train.values.ravel(),\n",
    "        cv=5,\n",
    "        scoring=['r2', 'neg_root_mean_squared_error', 'neg_mean_absolute_error'],\n",
    "        return_train_score=True,\n",
    "        params=fit_params\n",
    "    )\n",
    "    \n",
    "    result={\n",
    "        'Pipeline': pipe_name,\n",
    "        'Model': model_name,\n",
    "        'Train R2': cv_results['train_r2'].mean(),\n",
    "        'Test R2': cv_results['test_r2'].mean(),\n",
    "        'Test R2 Std': cv_results['test_r2'].std(),\n",
    "        'Test RMSE': -cv_results['test_neg_root_mean_squared_error'].mean(),\n",
    "        'Test MAE': -cv_results['test_neg_mean_absolute_error'].mean(),\n",
    "        'Best Params': 'Loaded from cache'\n",
    "    }\n",
    "    return result\n",
    "# Define models with hyperparameter grids\n",
    "model_configs = {\n",
    "    \"HistGB\": {\n",
    "        'model': HistGradientBoostingRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'model__max_iter': [300, 500],\n",
    "            'model__max_depth': [5, 8],\n",
    "            'model__learning_rate': [0.03, 0.05]\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        'model': XGBRegressor(\n",
    "            random_state=42,\n",
    "            objective='reg:squarederror',\n",
    "            tree_method='hist'\n",
    "        ),\n",
    "        'params': {\n",
    "            'model__n_estimators': [300],\n",
    "            'model__max_depth': [5, 7],\n",
    "            'model__learning_rate': [0.05]\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        'model': RandomForestRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'model__n_estimators': [200],\n",
    "            'model__max_depth': [15, 20],\n",
    "            'model__min_samples_leaf': [3, 5]\n",
    "        }\n",
    "    },\n",
    "    \"CatBoost\": {\n",
    "        \"model\": cb,\n",
    "        \"params\": {\n",
    "            \"model__iterations\": [500, 800],\n",
    "            \"model__depth\": [5, 6, 8],\n",
    "            \"model__learning_rate\": [0.03, 0.05],\n",
    "            \"model__l2_leaf_reg\": [3, 5, 7]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "pipelines = {\n",
    "    'pipe1_no_scaling': pipe1,\n",
    "    \"pipe1_optimized\": pipe1_optimized,\n",
    "    \"pipe1_optimized_scaled\": pipe1_optimized_scaled,\n",
    "    \"pipe2_no_grouper\": pipe2,\n",
    "    \"pipe2_optimized\": pipe2_optimized,\n",
    "    \"pipe3_with_grouper\": pipe3,\n",
    "    \"pipe4_target_enc\": pipe4,\n",
    "    \"pipe5_ratio_scaled\": pipe5_ratio_scaled,\n",
    "    \"pipe6_poly\": pipe6_poly,\n",
    "    \"pipe7_rare_moderate\": pipe7_rare_moderate,\n",
    "    \"catboost_pipe\": catboost_pipe,\n",
    "    \"catboost_pipe_nogroup\": catboost_pipe_nogroup\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "best_models = {}\n",
    "\n",
    "for pipe_name, pipe in pipelines.items():\n",
    "    pipelines[pipe_name] = pipe.set_output(transform=\"pandas\")\n",
    "    for model_name, config in model_configs.items():\n",
    "        print(f\"\\nTraining {model_name} with {pipe_name}...\")\n",
    "        \n",
    "        cache_path = build_cache_path(pipe_name, model_name)\n",
    "        if cache_path.exists():\n",
    "            print(f\"Loading cached model for {model_name} with {pipe_name}...\")\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                best_pipeline = pickle.load(f)\n",
    "            best_models[f\"{pipe_name}_{model_name}\"] = best_pipeline\n",
    "            results.append(evaluate_model(pipe_name, pipe, model_name, best_pipeline))\n",
    "            continue\n",
    "        \n",
    "        if model_name == \"CatBoost\" and pipe_name not in [\"catboost_pipe\", \"catboost_pipe_nogroup\"]:\n",
    "            print(f\"Skipping {model_name} with {pipe_name} due to incompatible preprocessing.\")\n",
    "            continue\n",
    "        if model_name != \"CatBoost\" and pipe_name in [\"catboost_pipe\", \"catboost_pipe_nogroup\"]:\n",
    "            print(f\"Skipping {model_name} with {pipe_name} due to incompatible preprocessing.\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        full_pipeline = Pipeline([\n",
    "            (\"preprocessing\", pipe),\n",
    "            (\"model\", config['model'])\n",
    "        ])\n",
    "        fit_params = {}\n",
    "        if model_name == \"CatBoost\":\n",
    "            pipe.fit(X_train, y_train)\n",
    "            feature_names = pipe.get_feature_names_out()\n",
    "            \n",
    "            cat_features_idx = [\n",
    "                i for i, col in enumerate(feature_names) \n",
    "                if \"cat_pipeline__\" in col\n",
    "            ]\n",
    "            \n",
    "            fit_params = {\n",
    "                'model__cat_features': cat_features_idx,\n",
    "            }\n",
    "\n",
    "        # Tune hyperparameters if params exist\n",
    "        if config['params']:\n",
    "            grid_search = GridSearchCV(\n",
    "                full_pipeline,\n",
    "                config['params'],\n",
    "                cv=5,\n",
    "                scoring='r2',\n",
    "                n_jobs=-1,\n",
    "                verbose=1,\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train.values.ravel(), **fit_params)\n",
    "            best_pipeline = grid_search.best_estimator_\n",
    "            best_params = grid_search.best_params_\n",
    "            # Cache the best model\n",
    "            cache_model(best_pipeline, pipe_name, model_name)\n",
    "        else:\n",
    "            best_pipeline = full_pipeline\n",
    "            best_params = {}\n",
    "        \n",
    "        # Cross-validation with best model\n",
    "        cv_results = cross_validate(\n",
    "            best_pipeline,\n",
    "            X_train,\n",
    "            y_train.values.ravel(),\n",
    "            cv=5,\n",
    "            scoring=['r2', 'neg_root_mean_squared_error', 'neg_mean_absolute_error'],\n",
    "            return_train_score=True,\n",
    "            params=fit_params\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Pipeline': pipe_name,\n",
    "            'Model': model_name,\n",
    "            'Train R2': cv_results['train_r2'].mean(),\n",
    "            'Test R2': cv_results['test_r2'].mean(),\n",
    "            'Test R2 Std': cv_results['test_r2'].std(),\n",
    "            'Test RMSE': -cv_results['test_neg_root_mean_squared_error'].mean(),\n",
    "            'Test MAE': -cv_results['test_neg_mean_absolute_error'].mean(),\n",
    "            'Best Params': str(best_params)\n",
    "        })\n",
    "        \n",
    "        # Store best model\n",
    "        best_models[f\"{pipe_name}_{model_name}\"] = best_pipeline\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Test R2', ascending=False)\n",
    "\n",
    "print(\"\\n=== Model Comparison Results (with Hyperparameter Tuning) ===\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find best combination\n",
    "best = results_df.iloc[0]\n",
    "print(f\"\\n=== Best Model ===\")\n",
    "print(f\"Pipeline: {best['Pipeline']}\")\n",
    "print(f\"Model: {best['Model']}\")\n",
    "print(f\"Test R2: {best['Test R2']:.4f} (Â±{best['Test R2 Std']:.4f})\")\n",
    "print(f\"Test MSE: {best['Test RMSE']:.2f}\")\n",
    "print(f\"Test MAE: {best['Test MAE']:.2f}\")\n",
    "print(f\"Best Params: {best['Best Params']}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "best_model_key = f\"{best['Pipeline']}_{best['Model']}\"\n",
    "final_model = best_models[best_model_key]\n",
    "y_pred = np.expm1(final_model.predict(X_test))\n",
    "test_r2 = r2_score(np.expm1(y_test), y_pred)\n",
    "test_mse = mean_squared_error(np.expm1(y_test), y_pred)\n",
    "\n",
    "print(f\"\\n=== Final Test Set Performance ===\")\n",
    "print(f\"Test R2: {test_r2:.4f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")\n",
    "\n",
    "        \n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fbbc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"model_comparison_results.csv\", index=False)\n",
    "results_df.to_json(\"model_comparison_results.json\", orient=\"records\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b4fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a profiling-only pipeline (imputation + scaling, but no OHE)\n",
    "profiling_pipe = ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"ratios\", ratio_adder),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_feature_cols),\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"rare_grouper\", RareLabelGrouper(tol=0.01, replace_with='other')),\n",
    "    ]), cat_feature_cols),\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "], remainder='drop').set_output(transform=\"pandas\")\n",
    "\n",
    "# Transform for profiling\n",
    "X_train_for_profiling = profiling_pipe.fit_transform(X_train)\n",
    "X_train_for_profiling['price'] = y_train.values\n",
    "\n",
    "# Generate profile\n",
    "profile_preprocessed = ProfileReport(X_train_for_profiling, title=\"Aqar Preprocessed Data Profiling Report (No OHE)\")\n",
    "profile_preprocessed.to_file(\"aqar_preprocessed_data_profiling_report.html\")\n",
    "print(\"Preprocessed data profiling report saved!\")\n",
    "print(f\"\\nProfiled data shape: {X_train_for_profiling.shape}\")\n",
    "print(\"Categorical columns preserved for easier interpretation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4644ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histograms of preprocessed features\n",
    "X_train_for_profiling.hist(bins=50, figsize=(20,15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w3 (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
