{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da91147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"aqar.csv\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d0301b",
   "metadata": {},
   "source": [
    "- Use Y-data\n",
    "- try multi layer preceptron(loss-funtion MSE, opt: adamw, Activation: Leaky ReLU)\n",
    "- try a Layered ML model(Use three diffrent ml models, they output to the forth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07504435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=50, figsize=(20,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944597f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_feature_cols=[\"area_sqm\",\"num_bathrooms\",\"num_bedrooms\",\"num_rooms\"]\n",
    "cat_feature_cols=[\"location\",]\n",
    "bool_feature_cols=[\"lift\"]\n",
    "\n",
    "df : pd.DataFrame = df[(df['is_rental'] == False) & (df['is_daily_rental'] == False) & (df['sale_type'] != 'rent') & (df['sale_type'] !='daily')].copy()\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "# drop listings of land without buildings\n",
    "df : pd.DataFrame = df[df['category_ga_property_category'] != 'land'].copy()\n",
    "# drop listings of commercial buildings\n",
    "df : pd.DataFrame = df[(df[\"category_ga_listing_type\"]!= \"office\") & (df[\"category_ga_listing_type\"]!=\"store\") & (df[\"category_ga_listing_type\"]!=\"warehouse\") & (df[\"category_ga_listing_type\"]!=\"lounge\")].copy()\n",
    "\n",
    "for bool_col in bool_feature_cols:\n",
    "    df[bool_col] = df[bool_col].astype(int)\n",
    "df['location'] = df['city'] + '_' + df['district']\n",
    "\n",
    "target_col=[\"price\"]\n",
    "\n",
    "\n",
    "df[num_feature_cols + cat_feature_cols + bool_feature_cols + target_col].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25689c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[num_feature_cols + cat_feature_cols + bool_feature_cols + target_col].hist(bins=50, figsize=(20,15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26111c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "profile = ProfileReport(df, title=\"All Aqar Data Profiling Report\", explorative=True)\n",
    "profile.to_file(\"raw_aqar_data_profiling_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbb1c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4502 is the number of rows \n",
    "# 1933 is the number of missing values in num_bathrooms\n",
    "df[\"num_bedrooms\"].isna().sum()\n",
    "df[num_feature_cols + cat_feature_cols + bool_feature_cols + target_col].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9dc1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile Data\n",
    "profile = ProfileReport(df[num_feature_cols + cat_feature_cols + bool_feature_cols + target_col].copy(), title=\"Aqar Dataset Profiling Report\")\n",
    "profile.to_file(\"subset_aqar_data_profiling_report.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df09689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training and Evaluation before preprocessing pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class RareLabelGrouper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, tol=0.01, replace_with='Other'):\n",
    "        self.tol = tol\n",
    "        self.replace_with = replace_with\n",
    "        self.frequent_labels_ = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Learn the frequent labels from the TRAINING set only\n",
    "        self.frequent_labels_ = {}\n",
    "        for col in X.columns:\n",
    "            # Calculate frequency\n",
    "            counts = pd.Series(X[col]).value_counts(normalize=True)\n",
    "            # Keep labels that are more frequent than tolerance\n",
    "            self.frequent_labels_[col] = counts[counts >= self.tol].index\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X).copy() # Ensure we don't modify original\n",
    "        for col in X.columns:\n",
    "            # Get the frequent labels we learned during fit\n",
    "            known_labels = self.frequent_labels_.get(col, [])\n",
    "            \n",
    "            # Apply the grouping\n",
    "            # If a value is NOT in known_labels, replace it with 'Other'\n",
    "            X[col] = X[col].where(X[col].isin(known_labels), self.replace_with)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a16ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler, TargetEncoder, PolynomialFeatures\n",
    "\n",
    "pipe1= ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ]), num_feature_cols),\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"rare_grouper\", RareLabelGrouper(tol=0.02, replace_with='other')),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\",sparse_output=False))\n",
    "    ]), cat_feature_cols),\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "])\n",
    "\n",
    "pipe2 = ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ]), num_feature_cols),\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"target_enc\", TargetEncoder())\n",
    "    ]), cat_feature_cols),\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "])\n",
    "\n",
    "pipe3 = ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ]), num_feature_cols),\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"rare_grouper\", RareLabelGrouper(tol=0.02, replace_with='other')),\n",
    "        (\"target_enc\", TargetEncoder())\n",
    "    ]), cat_feature_cols),\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "])\n",
    "pipe4 = ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ]), num_feature_cols),\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"target_enc\", TargetEncoder())\n",
    "    ]), cat_feature_cols),\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "catboost_pipe = ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ]), num_feature_cols),\n",
    "\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    ]), cat_feature_cols),\n",
    "\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "])\n",
    "\n",
    "\n",
    "# Define features and target\n",
    "X = df[num_feature_cols + cat_feature_cols + bool_feature_cols]\n",
    "y = np.log1p(df[target_col])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor,HistGradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "cb=CatBoostRegressor(loss_function=\"RMSE\",\n",
    "            eval_metric=\"R2\",\n",
    "            random_seed=42,\n",
    "            verbose=False\n",
    ")\n",
    "\n",
    "# Define models with hyperparameter grids\n",
    "model_configs = {\n",
    "    \"HistGB\": {\n",
    "        'model': HistGradientBoostingRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'model__max_iter': [300, 500],\n",
    "            'model__max_depth': [5, 8],\n",
    "            'model__learning_rate': [0.03, 0.05]\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        'model': XGBRegressor(\n",
    "            random_state=42,\n",
    "            objective='reg:squarederror',\n",
    "            tree_method='hist'\n",
    "        ),\n",
    "        'params': {\n",
    "            'model__n_estimators': [300],\n",
    "            'model__max_depth': [5, 7],\n",
    "            'model__learning_rate': [0.05]\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        'model': RandomForestRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'model__n_estimators': [200],\n",
    "            'model__max_depth': [15, 20],\n",
    "            'model__min_samples_leaf': [3, 5]\n",
    "        }\n",
    "    },\n",
    "    \"CatBoost\": {\n",
    "        \"model\": cb,\n",
    "        \"params\": {\n",
    "            \"model__iterations\": [500, 800],\n",
    "            \"model__depth\": [5, 6, 8],\n",
    "            \"model__learning_rate\": [0.03, 0.05],\n",
    "            \"model__l2_leaf_reg\": [3, 5, 7]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "pipelines = {\n",
    "    'pipe1_no_scaling': pipe1,\n",
    "    \"pipe2_no_grouper\": pipe2,\n",
    "    \"pipe3_with_grouper\": pipe3,\n",
    "    \"pipe4_target_enc\": pipe4,\n",
    "    \"catboost_pipe\": catboost_pipe\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "best_models = {}\n",
    "\n",
    "for pipe_name, pipe in pipelines.items():\n",
    "    pipelines[pipe_name] = pipe.set_output(transform=\"pandas\")\n",
    "    for model_name, config in model_configs.items():\n",
    "        print(f\"\\nTraining {model_name} with {pipe_name}...\")\n",
    "        \n",
    "        if model_name == \"CatBoost\" and pipe_name != \"catboost_pipe\":\n",
    "            print(f\"Skipping {model_name} with {pipe_name} due to incompatible preprocessing.\")\n",
    "            continue\n",
    "        if model_name != \"CatBoost\" and pipe_name == \"catboost_pipe\":\n",
    "            print(f\"Skipping {model_name} with {pipe_name} due to incompatible preprocessing.\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "        \n",
    "        # Create pipeline\n",
    "        full_pipeline = Pipeline([\n",
    "            (\"preprocessing\", pipe),\n",
    "            (\"model\", config['model'])\n",
    "        ])\n",
    "        fit_params = {}\n",
    "        if model_name == \"CatBoost\":\n",
    "            pipe.fit(X_train, y_train)\n",
    "            feature_names = pipe.get_feature_names_out()\n",
    "            \n",
    "            # 2. Identify indices of columns that came from 'cat_pipeline'\n",
    "            # (Matches the name you gave in ColumnTransformer)\n",
    "            cat_features_idx = [\n",
    "                i for i, col in enumerate(feature_names) \n",
    "                if \"cat_pipeline__\" in col\n",
    "            ]\n",
    "            \n",
    "            # 3. Pass these CORRECTED indices to the model fit params\n",
    "            fit_params = {\n",
    "                'model__cat_features': cat_features_idx,\n",
    "                # 'model__plot': True # Optional: View training plot\n",
    "            }\n",
    "\n",
    "        # Tune hyperparameters if params exist\n",
    "        if config['params']:\n",
    "            grid_search = GridSearchCV(\n",
    "                full_pipeline,\n",
    "                config['params'],\n",
    "                cv=5,\n",
    "                scoring='r2',\n",
    "                n_jobs=-1,\n",
    "                verbose=1,\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train.values.ravel(), **fit_params)\n",
    "            best_pipeline = grid_search.best_estimator_\n",
    "            best_params = grid_search.best_params_\n",
    "        else:\n",
    "            best_pipeline = full_pipeline\n",
    "            best_params = {}\n",
    "        \n",
    "        # Cross-validation with best model\n",
    "        cv_results = cross_validate(\n",
    "            best_pipeline,\n",
    "            X_train,\n",
    "            y_train.values.ravel(),\n",
    "            cv=5,\n",
    "            scoring=['r2', 'neg_root_mean_squared_error', 'neg_mean_absolute_error'],\n",
    "            return_train_score=True,\n",
    "            params=fit_params\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Pipeline': pipe_name,\n",
    "            'Model': model_name,\n",
    "            'Train R2': cv_results['train_r2'].mean(),\n",
    "            'Test R2': cv_results['test_r2'].mean(),\n",
    "            'Test R2 Std': cv_results['test_r2'].std(),\n",
    "            'Test RMSE': -cv_results['test_neg_root_mean_squared_error'].mean(),\n",
    "            'Test MAE': -cv_results['test_neg_mean_absolute_error'].mean(),\n",
    "            'Best Params': str(best_params)\n",
    "        })\n",
    "        \n",
    "        # Store best model\n",
    "        best_models[f\"{pipe_name}_{model_name}\"] = best_pipeline\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Test R2', ascending=False)\n",
    "\n",
    "print(\"\\n=== Model Comparison Results (with Hyperparameter Tuning) ===\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find best combination\n",
    "best = results_df.iloc[0]\n",
    "print(f\"\\n=== Best Model ===\")\n",
    "print(f\"Pipeline: {best['Pipeline']}\")\n",
    "print(f\"Model: {best['Model']}\")\n",
    "print(f\"Test R2: {best['Test R2']:.4f} (Â±{best['Test R2 Std']:.4f})\")\n",
    "print(f\"Test MSE: {best['Test RMSE']:.2f}\")\n",
    "print(f\"Test MAE: {best['Test MAE']:.2f}\")\n",
    "print(f\"Best Params: {best['Best Params']}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "best_model_key = f\"{best['Pipeline']}_{best['Model']}\"\n",
    "final_model = best_models[best_model_key]\n",
    "y_pred = np.expm1(final_model.predict(X_test))\n",
    "test_r2 = r2_score(np.expm1(y_test), y_pred)\n",
    "test_mse = mean_squared_error(np.expm1(y_test), y_pred)\n",
    "\n",
    "print(f\"\\n=== Final Test Set Performance ===\")\n",
    "print(f\"Test R2: {test_r2:.4f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")\n",
    "\n",
    "        \n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fbbc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"model_comparison_results.csv\", index=False)\n",
    "results_df.to_json(\"model_comparison_results.json\", orient=\"records\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0321b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# After you find your best params from the loop, define the fixed models\n",
    "estimators = [\n",
    "    ('cat', best_models['CatBoost_Pipe_CatBoost']),\n",
    "    ('xgb', best_models['OneHot_NoScaling_XGBoost']),\n",
    "    ('hist', best_models['TargetEnc_NoScaling_HistGB'])\n",
    "]\n",
    "\n",
    "stacking_reg = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=Ridge(alpha=1.0), # A simple linear model to combine predictions\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Fit this manually at the end\n",
    "print(\"Training Stacking Ensemble...\")\n",
    "stacking_reg.fit(X_train, y_train.values.ravel())\n",
    "score = stacking_reg.score(X_test, y_test)\n",
    "print(f\"Stacking R2: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b4fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a profiling-only pipeline (imputation + scaling, but no OHE)\n",
    "profiling_pipe = ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_feature_cols),\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    ]), cat_feature_cols),\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "], remainder='drop').set_output(transform=\"pandas\")\n",
    "\n",
    "# Transform for profiling\n",
    "X_train_for_profiling = profiling_pipe.fit_transform(X_train)\n",
    "X_train_for_profiling['price'] = y_train.values\n",
    "\n",
    "# Generate profile\n",
    "profile_preprocessed = ProfileReport(X_train_for_profiling, title=\"Aqar Preprocessed Data Profiling Report (No OHE)\")\n",
    "profile_preprocessed.to_file(\"aqar_preprocessed_data_profiling_report.html\")\n",
    "print(\"Preprocessed data profiling report saved!\")\n",
    "print(f\"\\nProfiled data shape: {X_train_for_profiling.shape}\")\n",
    "print(\"Categorical columns preserved for easier interpretation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4644ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histograms of preprocessed features\n",
    "X_train_for_profiling.hist(bins=50, figsize=(20,15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w3 (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
