{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da91147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"aqar.csv\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d0301b",
   "metadata": {},
   "source": [
    "- Use Y-data\n",
    "- try multi layer preceptron(loss-funtion MSE, opt: adamw, Activation: Leaky ReLU)\n",
    "- try a Layered ML model(Use three diffrent ml models, they output to the forth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07504435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=50, figsize=(20,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944597f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_feature_cols=[\"area_sqm\",\"num_bedrooms\",\"num_rooms\"]\n",
    "cat_feature_cols=[\"district\",\"city_grouped\"]\n",
    "bool_feature_cols=[\"lift\"]\n",
    "\n",
    "df : pd.DataFrame = df[(df['is_rental'] == False) & (df['is_daily_rental'] == False) & (df['sale_type'] != 'rent') & (df['sale_type'] !='daily')].copy()\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "# drop listings of land without buildings\n",
    "df : pd.DataFrame = df[df['category_ga_property_category'] != 'land'].copy()\n",
    "# drop listings of commercial buildings\n",
    "df : pd.DataFrame = df[(df[\"category_ga_listing_type\"]!= \"office\") & (df[\"category_ga_listing_type\"]!=\"store\") & (df[\"category_ga_listing_type\"]!=\"warehouse\") & (df[\"category_ga_listing_type\"]!=\"lounge\")].copy()\n",
    "\n",
    "for bool_col in bool_feature_cols:\n",
    "    df[bool_col] = df[bool_col].astype(int)\n",
    "\n",
    "# combine rare cities into 'Other'\n",
    "city_counts = df['city'].value_counts(normalize=True)\n",
    "rare_cities = city_counts[city_counts < 0.02].index\n",
    "df['city_grouped'] = df['city'].apply(lambda x: 'other' if x in rare_cities else x)\n",
    "\n",
    "target_col=[\"price\"]\n",
    "\n",
    "\n",
    "df[num_feature_cols + cat_feature_cols + bool_feature_cols + target_col].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25689c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[num_feature_cols + cat_feature_cols + bool_feature_cols + target_col].hist(bins=50, figsize=(20,15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26111c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbb1c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4502 is the number of rows \n",
    "# 1933 is the number of missing values in num_bathrooms\n",
    "df[\"num_bedrooms\"].isna().sum()\n",
    "df[num_feature_cols + cat_feature_cols + bool_feature_cols + target_col].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9dc1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile Data\n",
    "from ydata_profiling import ProfileReport\n",
    "profile = ProfileReport(df[num_feature_cols + cat_feature_cols + bool_feature_cols + target_col].copy(), title=\"Aqar Dataset Profiling Report\")\n",
    "profile.to_file(\"aqar_data_profiling_report.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df09689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training and Evaluation before preprocessing pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a16ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "pipe1= ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ]), num_feature_cols),\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]), cat_feature_cols),\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "])\n",
    "\n",
    "pipe2 = ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_feature_cols),\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]), cat_feature_cols),\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "])\n",
    "\n",
    "\n",
    "# Define features and target\n",
    "X = df[num_feature_cols + cat_feature_cols + bool_feature_cols]\n",
    "y = df[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Define models with hyperparameter grids\n",
    "model_configs = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'model__n_estimators': [50, 100, 200],\n",
    "            'model__max_depth': [None, 10, 20, 30],\n",
    "            'model__min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'LinearRegression': {\n",
    "        'model': LinearRegression(),\n",
    "        'params': {}  # No hyperparameters to tune\n",
    "    },\n",
    "    'SVR': {\n",
    "        'model': SVR(),\n",
    "        'params': {\n",
    "            'model__C': [0.1, 1, 10],\n",
    "            'model__kernel': ['rbf', 'linear'],\n",
    "            'model__gamma': ['scale', 'auto']\n",
    "        }\n",
    "    },\n",
    "    'KNN': {\n",
    "        'model': KNeighborsRegressor(),\n",
    "        'params': {\n",
    "            'model__n_neighbors': [3, 5, 7, 10],\n",
    "            'model__weights': ['uniform', 'distance']\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBRegressor(random_state=42, objective='reg:squarederror'),\n",
    "        'params': {\n",
    "            'model__n_estimators': [50, 100, 200],\n",
    "            'model__max_depth': [3, 5, 7],\n",
    "            'model__learning_rate': [0.01, 0.1, 0.3]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "pipelines = {\n",
    "    'pipe1_no_scaling': pipe1,\n",
    "    'pipe2_with_scaling': pipe2\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "best_models = {}\n",
    "\n",
    "for pipe_name, pipe in pipelines.items():\n",
    "    for model_name, config in model_configs.items():\n",
    "        print(f\"\\nTraining {model_name} with {pipe_name}...\")\n",
    "        \n",
    "        # Create pipeline\n",
    "        full_pipeline = Pipeline([\n",
    "            (\"preprocessing\", pipe),\n",
    "            (\"model\", config['model'])\n",
    "        ])\n",
    "        \n",
    "        # Tune hyperparameters if params exist\n",
    "        if config['params']:\n",
    "            grid_search = GridSearchCV(\n",
    "                full_pipeline,\n",
    "                config['params'],\n",
    "                cv=5,\n",
    "                scoring='r2',\n",
    "                n_jobs=-1,\n",
    "                verbose=1\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train.values.ravel())\n",
    "            best_pipeline = grid_search.best_estimator_\n",
    "            best_params = grid_search.best_params_\n",
    "        else:\n",
    "            best_pipeline = full_pipeline\n",
    "            best_params = {}\n",
    "        \n",
    "        # Cross-validation with best model\n",
    "        cv_results = cross_validate(\n",
    "            best_pipeline, \n",
    "            X_train, \n",
    "            y_train.values.ravel(),\n",
    "            cv=5,\n",
    "            scoring=['r2', 'neg_mean_squared_error', 'neg_mean_absolute_error'],\n",
    "            return_train_score=True\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Pipeline': pipe_name,\n",
    "            'Model': model_name,\n",
    "            'Train R2': cv_results['train_r2'].mean(),\n",
    "            'Test R2': cv_results['test_r2'].mean(),\n",
    "            'Test R2 Std': cv_results['test_r2'].std(),\n",
    "            'Test MSE': -cv_results['test_neg_mean_squared_error'].mean(),\n",
    "            'Test MAE': -cv_results['test_neg_mean_absolute_error'].mean(),\n",
    "            'Best Params': str(best_params)\n",
    "        })\n",
    "        \n",
    "        # Store best model\n",
    "        best_models[f\"{pipe_name}_{model_name}\"] = best_pipeline\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Test R2', ascending=False)\n",
    "\n",
    "print(\"\\n=== Model Comparison Results (with Hyperparameter Tuning) ===\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find best combination\n",
    "best = results_df.iloc[0]\n",
    "print(f\"\\n=== Best Model ===\")\n",
    "print(f\"Pipeline: {best['Pipeline']}\")\n",
    "print(f\"Model: {best['Model']}\")\n",
    "print(f\"Test R2: {best['Test R2']:.4f} (Â±{best['Test R2 Std']:.4f})\")\n",
    "print(f\"Test MSE: {best['Test MSE']:.2f}\")\n",
    "print(f\"Test MAE: {best['Test MAE']:.2f}\")\n",
    "print(f\"Best Params: {best['Best Params']}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "best_model_key = f\"{best['Pipeline']}_{best['Model']}\"\n",
    "final_model = best_models[best_model_key]\n",
    "y_pred = final_model.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n=== Final Test Set Performance ===\")\n",
    "print(f\"Test R2: {test_r2:.4f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b4fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a profiling-only pipeline (imputation + scaling, but no OHE)\n",
    "profiling_pipe = ColumnTransformer([\n",
    "    (\"num_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), num_feature_cols),\n",
    "    (\"cat_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    ]), cat_feature_cols),\n",
    "    (\"bool_pipeline\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    ]), bool_feature_cols)\n",
    "], remainder='drop').set_output(transform=\"pandas\")\n",
    "\n",
    "# Transform for profiling\n",
    "X_train_for_profiling = profiling_pipe.fit_transform(X_train)\n",
    "X_train_for_profiling['price'] = y_train.values\n",
    "\n",
    "# Generate profile\n",
    "profile_preprocessed = ProfileReport(X_train_for_profiling, title=\"Aqar Preprocessed Data Profiling Report (No OHE)\")\n",
    "profile_preprocessed.to_file(\"aqar_preprocessed_data_profiling_report.html\")\n",
    "print(\"Preprocessed data profiling report saved!\")\n",
    "print(f\"\\nProfiled data shape: {X_train_for_profiling.shape}\")\n",
    "print(\"Categorical columns preserved for easier interpretation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4644ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histograms of preprocessed features\n",
    "X_train_for_profiling.hist(bins=50, figsize=(20,15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w3 (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
